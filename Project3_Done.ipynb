{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eqr7Mv0iG2A4"
   },
   "source": [
    "# **Project 3: Movie Review Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irg-jtBw7ZPN"
   },
   "source": [
    "\n",
    "Team Contribution:\n",
    "\n",
    "\n",
    "*  Christine Zhou     netID: xizhou4   UIN: *****6213 Online MCS\n",
    "   - Contributions     Part I Build a Binary Classification Model\n",
    "\n",
    "\n",
    "*  Syed Ahmed         netID: syeda2    UIN: *****5315 Online MCS\n",
    "   - Contributions     Part II Interpretability Analysis\n",
    "\n",
    "\n",
    "*  Jessica Tomas      netID: jptomas2  UIN: *****0877 Online MCS\n",
    "   - Contributions     Part II Interpretability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISEUyhYm4lNr"
   },
   "source": [
    "\n",
    "\n",
    "# **1. Build a Binary Classification Model**\n",
    "\n",
    "The first objective is to construct a binary classification model to predict the sentiment of a movie review.\n",
    "\n",
    "The evaluation metric for this project is the Area Under the Curve (AUC) on the test data. Your goal is to achieve an AUC score of at least 0.986 across all five test data splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "xghMidF-4lNr"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "M1zL3ZQ-4lNs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "system_specs = \"Macbook Pro, 3.49 GHz, 24GB memory\"\n",
    "execution_times = []\n",
    "\n",
    "# Load the data in jupyter notebook\n",
    "def load_data(split_num):\n",
    "    train_df = pd.read_csv(f'split_{split_num}/train.csv')\n",
    "    test_df = pd.read_csv(f'split_{split_num}/test.csv')\n",
    "    test_labels = pd.read_csv(f'split_{split_num}/test_y.csv')\n",
    "\n",
    "    return train_df, test_df, test_labels\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(data):\n",
    "    data['review'] = data['review'].str.replace('<.*?>', ' ', regex=True)\n",
    "    return data\n",
    "\n",
    "# Train a model with embeddings      C_value= 8.0, 10.0, 12.0\n",
    "def train_model(X_train, y_train, C_value=10.0, solver='liblinear', use_embeddings=True):\n",
    "    \"\"\"\n",
    "    Train a logistic regression model using provided training data.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: Training feature data (NumPy array or DataFrame).\n",
    "    - y_train: Training labels.\n",
    "    - C_value: Regularization strength for Logistic Regression.\n",
    "    - solver: Solver to use in Logistic Regression.\n",
    "    - use_embeddings: Ignored if X_train is already prepared.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained Logistic Regression model.\n",
    "    \"\"\"\n",
    "    # Assume X_train is already a NumPy array; no further slicing required\n",
    "    model = LogisticRegression(random_state=42, solver=solver, max_iter=2000, C=C_value)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "GLJun8bx4lNt"
   },
   "outputs": [],
   "source": [
    "# Evaluate the  model using AUC\n",
    "def evaluate_model(model, X_test, y_test, use_embeddings=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model using the AUC metric on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained model.\n",
    "    - X_test: The test feature data (DataFrame or NumPy array).\n",
    "    - y_test: The actual labels for the test data.\n",
    "    - use_embeddings: Whether to use the last 1536 embedding columns.\n",
    "\n",
    "    Returns:\n",
    "    - AUC score: The Area Under the Curve score for the test data.\n",
    "    \"\"\"\n",
    "    if use_embeddings and isinstance(X_test, pd.DataFrame):\n",
    "        # Use only the last 1536 columns if embeddings are included\n",
    "        X_test = X_test.iloc[:, -1536:].values\n",
    "    predictions = model.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "    return roc_auc_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "8-hyeyAl4lNt",
    "outputId": "078e8630-299a-48ae-de81-39e7fb5ca8ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 1 -  AUC Score: 0.987113392676245,     Execution Time - 24.539942741394043 seconds\n",
      "Split 2 -  AUC Score: 0.9868045409410647,     Execution Time - 23.985557079315186 seconds\n",
      "Split 3 -  AUC Score: 0.9864294594933899,     Execution Time - 24.255937099456787 seconds\n",
      "Split 4 -  AUC Score: 0.9869832812693,     Execution Time - 23.31607437133789 seconds\n",
      "Split 5 -  AUC Score: 0.9862851395212046,     Execution Time - 25.736347913742065 seconds\n",
      "Average Validation AUC across all splits: 0.986723162780241\n",
      "Average Execution Time: 24.37 seconds on Macbook Pro, 3.49 GHz, 24GB memory\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    #  Try using different hyper-parameter\n",
    "    # C_value = (8.0, 10.0, 12.0)  solver =(liblinear , saga, lbfgs  # test_size = 0.1, 0.2, 0.3\n",
    "\n",
    "    auc_scores = []\n",
    "    for split_num in range(1, 6):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Load and preprocess data\n",
    "        train_data, test_data , test_labels = load_data(split_num)\n",
    "        train_data = preprocess_data(train_data)\n",
    "        test_data = preprocess_data(test_data)\n",
    "\n",
    "        # Prepare training and test data\n",
    "        X_train = train_data.iloc[:, -1536:].values  # Converts DataFrame to NumPy array\n",
    "        y_train = train_data['sentiment']\n",
    "        X_test = test_data.iloc[:, -1536:]  # Keep X_test as a DataFrame\n",
    "        y_test = test_labels['sentiment']\n",
    "\n",
    "        # Train and evaluate model\n",
    "        model = train_model(X_train, y_train, C_value=10.0, solver='liblinear')\n",
    "        X_train = train_data.iloc[:, -1536:].values  # Converts DataFrame to NumPy array\n",
    "        auc = evaluate_model(model, X_test, y_test )  # Pass the DataFrame X_test\n",
    "        auc_scores.append(auc)\n",
    "\n",
    "        execution_time = time.time() - start_time\n",
    "        execution_times.append(execution_time)\n",
    "\n",
    "        print(f\"Split {split_num} -  AUC Score: {auc},     Execution Time - {execution_time} seconds\")\n",
    "\n",
    "    # Calculate average AUC across all splits\n",
    "    avg_auc = np.mean(auc_scores)\n",
    "    print(\"Average Validation AUC across all splits:\", avg_auc)\n",
    "    print(f\"Average Execution Time: {np.mean(execution_times):.2f} seconds on {system_specs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yu9FrtdCCiHd"
   },
   "source": [
    "###  Conclusion for Build a Binary Classification Model\n",
    "\n",
    "\n",
    "The objective of designing a binary classification model to predict movie review sentiment, was successfully met through methodical data preprocessing, the utilization of pre-trained embeddings, and logistic regression. The model achieved impressive Area Under Curve (AUC) scores exceeding the 0.9867 benchmark across all five test data splits, demonstrating robust performance and a high degree of predictive accuracy. Preprocessing involved cleaning and leveraging OpenAI embeddings directly, highlighting an efficient approach to handling large-scale textual data. This approach's effectiveness was evidenced by consistently high AUC scores, with an average of approximately 0.987 across splits. The logistic regression models were tuned with varying parameters to optimize performance, and the training and evaluation processes were computationally efficient, running smoothly on a MacBook Pro 3.49 GHz, 24GB memory setup. These results illustrate the model’s capability to discern sentiment accurately, making it a valuable tool for large-scale sentiment analysis in real-world applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7O2X7XvwH8si"
   },
   "source": [
    "# **2. Interpretability Analysis**\n",
    "\n",
    "Using split 1 and the corresponding trained model, implement an interpretability approach to identify which parts of each review have an impact on the sentiment prediction. Apply your method to 5 randomly selected positive reviews and 5 randomly selected negative reviews from the split 1 test data.\n",
    "\n",
    "Set a random seed before selecting these 10 reviews (the seed does not need to relate to students’ UINs).\n",
    "\n",
    "Provide visualizations (such as highlighted text) that show the key parts of a review contributing to the sentiment prediction. Discuss the effectiveness and limitations of the interpretability approach you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6E2IuHGnxTcD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "fOVrXD__4lNv"
   },
   "outputs": [],
   "source": [
    "# load the model from Part 1\n",
    "url = \"https://github.com/syedmustafaahmed/PSL-project-3/raw/refs/heads/main/trained_model.pkl\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "# Load the model directly from the response content\n",
    "model_from_github = joblib.load(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UUshETSl4lNv"
   },
   "outputs": [],
   "source": [
    "# load the embeddings from Part 1\n",
    "url = \"https://github.com/syedmustafaahmed/PSL-project-3/raw/refs/heads/main/test_scaled.pkl\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "# Load the model directly from the response content\n",
    "test_scaled_from_github = joblib.load(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "pUMmyHu74lNw"
   },
   "outputs": [],
   "source": [
    "# load the vocab words from Part 1\n",
    "url = \"https://github.com/syedmustafaahmed/PSL-project-3/raw/refs/heads/main/vocab_words.pkl\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "# Load the model directly from the response content\n",
    "vocab_words_from_github = joblib.load(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "MffkCuoX4lNw"
   },
   "outputs": [],
   "source": [
    "# load the dtm_test from Part 1\n",
    "url = \"https://github.com/syedmustafaahmed/PSL-project-3/raw/refs/heads/main/dtm_test.pkl\"\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check for HTTP request errors\n",
    "\n",
    "# Load the model directly from the response content\n",
    "dtm_test_from_github = joblib.load(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "6N1Bm16R4lNw"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(f'split_1/test.csv')\n",
    "test_df = test_df.drop(columns=['id', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "x4UlF5DM4lNw"
   },
   "outputs": [],
   "source": [
    "# transform dtm_test into OpenAI embeddings format, so that we can then use the model from part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "Tex1IQjP4lNw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 38s, sys: 20.9 s, total: 10min 59s\n",
      "Wall time: 21min 13s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-2 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-2 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-2 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-2 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-2 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-2 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-2 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-2 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-2 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LinearRegression.html\">?<span>Documentation for LinearRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mapping_model = LinearRegression()\n",
    "mapping_model.fit(test_scaled_from_github, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "74gIMGxj4lNw"
   },
   "outputs": [],
   "source": [
    "mapping_predicted = mapping_model.predict(test_scaled_from_github)\n",
    "model_from_github_predictions = model_from_github.predict(mapping_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "uFegU3bl4lNw"
   },
   "outputs": [],
   "source": [
    "Y_test = pd.read_csv('split_1/test_y.csv')\n",
    "Y_test = Y_test['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmMSMw19xTcK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89692\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = accuracy_score(Y_test, model_from_github_predictions)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ML4CSO2xxTcK"
   },
   "outputs": [],
   "source": [
    "# now need to get five positive test examples from split 1, and five negative test examples from split 1\n",
    "X_test_split_1 = pd.read_csv(f\"./split_1/test.csv\")\n",
    "Y_test_split_1 = pd.read_csv(f\"./split_1/test_y.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3e8CmaLbxTcK"
   },
   "outputs": [],
   "source": [
    "one_indexes = Y_test_split_1[Y_test_split_1['sentiment'] == 1].index.tolist()\n",
    "zero_indexes = Y_test_split_1[Y_test_split_1['sentiment'] == 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufUuskDCxTcK"
   },
   "outputs": [],
   "source": [
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Zkuru9rxTcK"
   },
   "outputs": [],
   "source": [
    "random_one_indexes = random.sample(one_indexes, 5)\n",
    "random_zero_indexes = random.sample(zero_indexes, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P56dyvnFxTcK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(model_from_github_predictions[random_one_indexes])\n",
    "print(model_from_github_predictions[random_zero_indexes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8TWGQxCxTcK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4347     This is an account of events that have been co...\n",
       "18830    I had the good fortune to be at Perris Island ...\n",
       "2017     Las Vegas is one of the most brilliant shows o...\n",
       "8452     Having Just \\Welcomed Home\\\" my 23 YR old daug...\n",
       "3803     i watched this series when it first came out i...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_reviews = X_test_split_1.iloc[random_one_indexes]['review']\n",
    "positive_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MI1ExDLxTcK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16077    Contains spoilers The movie plot can be summar...\n",
       "24883    I hated this crap, every Friday as part of tgi...\n",
       "14583    Trite and tiring, the one-liners almost made m...\n",
       "15319    I admit to liking a lot of the so-called \\frat...\n",
       "21192    Now we know where they got the idea of Snakes ...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_reviews = X_test_split_1.iloc[random_zero_indexes]['review']\n",
    "negative_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kv221dAaxTcK"
   },
   "outputs": [],
   "source": [
    "embeddings_positive = dtm_test_from_github[random_one_indexes].toarray()\n",
    "embeddings_negative = dtm_test_from_github[random_zero_indexes].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahW1tZQ3xTcK"
   },
   "outputs": [],
   "source": [
    "# loop over each review and find the words that contributed to sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gTt3s16T4lNx"
   },
   "outputs": [],
   "source": [
    "negative_words_in_reviews = []\n",
    "positive_words_in_reviews = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwK0Z16txTcL"
   },
   "outputs": [],
   "source": [
    "for review in embeddings_positive:\n",
    "    positive_words = []\n",
    "    for i in range(len(review)):\n",
    "        if review[i] == 1:\n",
    "            positive_words.append(vocab_words_from_github[i])\n",
    "    positive_words_in_reviews.append(positive_words)\n",
    "\n",
    "for review in embeddings_negative:\n",
    "    negative_words = []\n",
    "    for i in range(len(review)):\n",
    "        if review[i] == 1:\n",
    "            negative_words.append(vocab_words_from_github[i])\n",
    "    negative_words_in_reviews.append(negative_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZeWMHiuxTcL"
   },
   "outputs": [],
   "source": [
    "negative_reviews = negative_reviews.values\n",
    "positive_reviews = positive_reviews.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFb4F-R_xTcL"
   },
   "outputs": [],
   "source": [
    "# now visualize the positive reviews (use green for text that has positive sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLkVQulixTcL"
   },
   "outputs": [],
   "source": [
    "def highlight_words(color, text, words_to_highlight):\n",
    "    highlighted_words = [re.escape(word) for word in words_to_highlight]\n",
    "    pattern = r'\\b(' + '|'.join(highlighted_words) + r')\\b'\n",
    "    print(re.sub(pattern, color, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gFHUPBmPxTcL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Review 1:\n",
      "This is an account of events that have been covered in print several times, and I had read two books - 'A Voyage for Madmen' and 'The Strange Last Voyage of Donald Crowhurst' before seeing the film in Sheffield just before Christmas. I must \u001b[1;92msay\u001b[0m, it exceeded all expectations in its telling of the 1968 Sunday Times Golden Globe yacht race. These men set out \u001b[1;92mto do\u001b[0m \u001b[1;92msomething\u001b[0m that had never been done before \u001b[1;92mwith no\u001b[0m support vessels, \u001b[1;92mwooden\u001b[0m boats, no satellite phones, no GPS, and just their wits and skill to \u001b[1;92mget\u001b[0m them round the globe in one piece. Not to \u001b[1;92mmention\u001b[0m the months of solitude, the thundering southern ocean, little sleep, and boats that were \u001b[1;92moften\u001b[0m literally falling apart around them.<br /><br />This documentary is \u001b[1;92mexcellently\u001b[0m put \u001b[1;92mtogether\u001b[0m in my opinion, tightly edited, \u001b[1;92mwell\u001b[0m paced with \u001b[1;92msuperb\u001b[0m narration. The archive footage and the interviews are \u001b[1;92mfascinating\u001b[0m and bring the story \u001b[1;92mto life\u001b[0m. Clare Crowhurst's interview footage is \u001b[1;92mespecially\u001b[0m revealing and \u001b[1;92mmoving\u001b[0m as she relates the events that led up to her husband, Donald Crowhurst's departure from Teignmouth, the doubts and fears in his mind and her reaction as subsequent events unfolded.<br /><br />I was \u001b[1;92mmoved\u001b[0m and had \u001b[1;92meven\u001b[0m shed a tear \u001b[1;92mor\u001b[0m two \u001b[1;92mby\u001b[0m the time the credits started rolling, and overheard other people expressing similar \u001b[1;92mfeelings\u001b[0m.<br /><br />The two books I mentioned above are useful for more detail and back-story which \u001b[1;92mcouldn't\u001b[0m have been fitted into the \u001b[1;92m90\u001b[0m \u001b[1;92mminutes\u001b[0m and I \u001b[1;92mwould\u001b[0m \u001b[1;92mrecommend\u001b[0m those too.<br /><br />This is ultimately a \u001b[1;92mtrue\u001b[0m \u001b[1;92mstory of\u001b[0m human \u001b[1;92mcourage\u001b[0m and human frailty. A \u001b[1;92mmust see\u001b[0m for anyone interested in sailing, \u001b[1;92madventure\u001b[0m, human endeavour and real-life heroes.\n",
      "\n",
      "Positive Review 2:\n",
      "I \u001b[1;92mhad\u001b[0m the good fortune \u001b[1;92mto be\u001b[0m at Perris Island in the fall of 1959. The DI showed one evening at the outdoor theater directly in front of our barracks, Plt 162, B Co, 1st Bn, 1st ITR.<br /><br />Although we hadn't \u001b[1;92mbeen\u001b[0m \u001b[1;92mthere\u001b[0m long \u001b[1;92menough\u001b[0m \u001b[1;92mto even\u001b[0m think about seeing a movie, we \u001b[1;92mcould\u001b[0m hear those that were laughing. It's \u001b[1;92mone of\u001b[0m the \u001b[1;92mmany\u001b[0m indelible \u001b[1;92mmemories\u001b[0m of my thirteen weeks at PI.<br /><br />At \u001b[1;92msome\u001b[0m \u001b[1;92mlater\u001b[0m date, I \u001b[1;92mgot\u001b[0m to \u001b[1;92mactually\u001b[0m \u001b[1;92msee\u001b[0m it in a theater. I'm \u001b[1;92mstill\u001b[0m convinced that, to date, it remains the \u001b[1;92mmost\u001b[0m \u001b[1;92mrealistic\u001b[0m \u001b[1;92mportrayal\u001b[0m of the \u001b[1;92mexperience\u001b[0m in the \u001b[1;92mlate\u001b[0m 1950's \u001b[1;92mever\u001b[0m done. No one has done it \u001b[1;92mbetter\u001b[0m than Jack Webb...\n",
      "\n",
      "Positive Review 3:\n",
      "Las Vegas is one of the \u001b[1;92mmost\u001b[0m \u001b[1;92mbrilliant\u001b[0m shows of our time, its combines hard-hitting action with light drama and heavy doses of comedy. It features \u001b[1;92mfantastic\u001b[0m characters lead by the charismatic tough-\u001b[1;92mguy\u001b[0m Ed Deline (James Caan). The show uses cool high-tech surveillance equipment to bring down the cheats and schemers. The characters are joyful to watch \u001b[1;92mespecially\u001b[0m as their \u001b[1;92mdifferent\u001b[0m departments within the hotel/casino cross paths.<br /><br />The show is mainly centred around the surveillance and security part of the hotel/casino. The two leading characters are Ed Deline, president of operations and Danny McCoy (Josh Duhamel), former US Marine who served in Iraq but is now head of security. The shows five other main characters are former valet now security personnel MIT graduate Mike Cannon (James Lesure); the feisty, sexy casino host Samanthat Marquez (Vanessa Marcil); Danny's \u001b[1;92mchildhood\u001b[0m sweetheart hotel manager Mary O'Connell (Nikki Cox); Danny's current sweetheart also Big Ed's daughter and manager of Mystique Delinda Deline (Molly Sims) and Ed's adopted daughter from his CIA \u001b[1;92mpast\u001b[0m casino floor manager Nessa Holt (Marsha Thomason) who left the show after the second season. Each character is \u001b[1;92munique\u001b[0m in their \u001b[1;92mown\u001b[0m quirky way, giving the show its energy and charisma that keeps its audience entertained for the \u001b[1;92mentire\u001b[0m duration of an episode.<br /><br />Every episode features a special quest star \u001b[1;92meither\u001b[0m a singer, actor \u001b[1;92mor\u001b[0m band who perform at the hotel's nightclub Mystique. These cameo appearances by big names is a specialty that is popular \u001b[1;92mamong\u001b[0m the shows audience.<br /><br />Las Vegas is a show that can appeal to \u001b[1;92mboth\u001b[0m male and female audiences. For the guys the show features sexy women, classy sports cars, high stakes gambling, adrenaline pumping action and overall a pool for topless women. For the ladies \u001b[1;92mthere\u001b[0m is the \u001b[1;92myoung\u001b[0m and handsome leading \u001b[1;92mman\u001b[0m (Duhamel) and the tough edgy Deline, romance and tanned topless guys around the same pool.<br /><br />The \u001b[1;92mplot\u001b[0m of each episode combines action with light drama and comedy to break the ice. Each \u001b[1;92mepisodes\u001b[0m also features \u001b[1;92m2\u001b[0m \u001b[1;92meven\u001b[0m \u001b[1;92m3\u001b[0m secondary stories which revolve mainly around the female characters. Every season also ends with a bang which leaves the audience hanging until the airing of the next season. An \u001b[1;92mexcellent\u001b[0m way to end a season! With a mix of genre's driving the show and a \u001b[1;92mcast\u001b[0m of colourful, charismatic characters and of course lets not forget the topless pool \u001b[1;92mmakes\u001b[0m Las Vegas one of the \u001b[1;92mgreatest\u001b[0m TV shows \u001b[1;92mever\u001b[0m aired.<br /><br />\u001b[1;92m9\u001b[0m/10\n",
      "\n",
      "Positive Review 4:\n",
      "Having Just \\Welcomed Home\\\" my 23 YR old daughter from a year in Iraq, Camp Anaconda medical support unit, I felt compelled to \u001b[1;92mget\u001b[0m this DVD. I \u001b[1;92mwanted to\u001b[0m hear other returning vets \u001b[1;92mfeelings\u001b[0m in order to \u001b[1;92mattempt\u001b[0m to \u001b[1;92mbetter\u001b[0m understand her mentality on arrival and not waiting until after \u001b[1;92msomething\u001b[0m \u001b[1;92mbad\u001b[0m happened. Regardless on your take on the war and peace this movie serves as a great start for all Americans to begin the healing of our returning vets emotional void. The paramount statement of the \u001b[1;92mentire movie\u001b[0m is \\\"Take Action\\\" on the \u001b[1;92mproblem\u001b[0m . Incredibly emotional movie. I \u001b[1;92mwould\u001b[0m \u001b[1;92mhighly\u001b[0m \u001b[1;92mrecommend\u001b[0m this movie to the vet the vets entire mature \u001b[1;92mfamily\u001b[0m and ask that they follow through with a plan to listen comfort help the returning Gulf War Enduring Freedom vets.<br /><br />Fast forward nearly one year \u001b[1;92mlater\u001b[0m & My daughter has seen this DVD. Took account of her \u001b[1;92memotions\u001b[0m and \u001b[1;92mactually\u001b[0m has \u001b[1;92mmade\u001b[0m a commitment to re-up for another 6 \u001b[1;92myears\u001b[0m. Her take on her time \u001b[1;92mspent\u001b[0m in the sand is that she \u001b[1;92mdid\u001b[0m some good. Local Balad children \u001b[1;92mgot\u001b[0m \u001b[1;92mfirst rate\u001b[0m medical treatment for various common ailments not ordinarily able to afford free with an escort and translator. Her look over her shoulder at her Iraq tour was . \\\"We changed some hearts and minds back \u001b[1;92mthere\u001b[0m\\\" Great DVD you have to keep an open mind and \u001b[1;92msee\u001b[0m all sides\"\n",
      "\n",
      "Positive Review 5:\n",
      "i watched this series when it first came out in the 70s.i was 14 years old and i watched it at my best friends house as my dad \u001b[1;92mdidn't\u001b[0m \u001b[1;92mwant\u001b[0m to watch it.it became a weekly ritual every Sunday, and as anyone will tell you for two fourteen \u001b[1;92myear olds\u001b[0m to watch a documentary in almost reverential silence must \u001b[1;92mmean\u001b[0m \u001b[1;92mthat this\u001b[0m was \u001b[1;92msomething\u001b[0m special.<br /><br />the broad sweep of the events of \u001b[1;92mworld\u001b[0m war \u001b[1;92m2\u001b[0m \u001b[1;92mmakes\u001b[0m for a difficult subject to document.so the makers broke it down into what they considered \u001b[1;92mto be\u001b[0m the most significant key happenings and devoted one episode to \u001b[1;92meach\u001b[0m.\u001b[1;92msome\u001b[0m episodes covered long periods such as 'wolf pack' which covered nearly all six years of the battle of the Atlantic.while the battle of Stalingrad had one episode to itself.<br /><br />this documentary \u001b[1;92mcould\u001b[0m \u001b[1;92mnot be\u001b[0m made \u001b[1;92mtoday\u001b[0m \u001b[1;92mquite\u001b[0m simply \u001b[1;92mbecause\u001b[0m most of those interviewed are dead.the list of significant players appearing \u001b[1;92mgives\u001b[0m an \u001b[1;92mamazing\u001b[0m insight into the \u001b[1;92mthinking\u001b[0m at the time.Anthony eden the foreign secretary,Carl donnitz,head of the u-boats,Albert speer,pet architect confident and later armament minister for Hitler.in \u001b[1;92mone of\u001b[0m the later episodes we \u001b[1;92msee\u001b[0m traudl junge, Hitler's secretary,\u001b[1;92mwho\u001b[0m was with him in the bunker and it was to her that he dictated his last will and testament-she left the bunker after Hitler's suicide and escaped through the Russian \u001b[1;92mlines\u001b[0m.these and \u001b[1;92mmany\u001b[0m \u001b[1;92mothers\u001b[0m play a major \u001b[1;92mrole\u001b[0m in the realism of the events portrayed.<br /><br />\u001b[1;92mif\u001b[0m i have \u001b[1;92many\u001b[0m criticism of the series it is that the code-breakers of bletchly park are not included but the revelations of their part in the war \u001b[1;92monly\u001b[0m emerged after the series had \u001b[1;92mbeen\u001b[0m made so i cannot \u001b[1;92mblame\u001b[0m the programme makers.<br /><br />the opening titles and \u001b[1;92mmusic\u001b[0m are \u001b[1;92mmagnificent\u001b[0m,and Lawrence Olivier's narration lends a \u001b[1;92mnatural\u001b[0m gravity to the \u001b[1;92mscript\u001b[0m.<br /><br />the best documentary series \u001b[1;92mever\u001b[0m made? without doubt.unmissable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(positive_reviews)):\n",
    "    print(f'Positive Review {i+1}:')\n",
    "    positive_review = positive_reviews[i]\n",
    "    highlight_words(r'\\033[1;92m\\1\\033[0m', positive_review, positive_words_in_reviews[i])\n",
    "    # highlight_words(r'\\033[1;92m\\1\\032[0m', positive_review, positive_words_in_reviews[i])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4Sk8s19xTcL"
   },
   "outputs": [],
   "source": [
    "# now visualize the negative reviews (use bright red for text that has negative sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r7y8vEjpxTcL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Review 0:\n",
      "Contains spoilers The movie \u001b[1;91mplot\u001b[0m can be summarized in a few sentences: Three guys go hunting in the forest. Two of them along other people \u001b[1;91mget\u001b[0m shot in the head without explanation. The last guy can stand in the clear, shout and \u001b[1;91mdo\u001b[0m \u001b[1;91manything\u001b[0m without getting shot. He gets to walk through an old factory and has the evil people walk right into his scope without a struggle. The villains are conveniently dressed in black and \u001b[1;91mlook like\u001b[0m villains.<br /><br />That is the \u001b[1;91mwhole\u001b[0m \u001b[1;91mstory\u001b[0m, not summarized but in detail. Everything is drawn out with a guy standing ringing a door bell. We wait with him. Long shot of guys being \u001b[1;91mbored\u001b[0m in the woods and sleeping. We can take a nap with them. The one drawn out shot of following a female jogger could have \u001b[1;91mbeen\u001b[0m \u001b[1;91mredeeming\u001b[0m, if we could \u001b[1;91msee\u001b[0m her butt \u001b[1;91mor\u001b[0m boobs bouncing.<br /><br />There \u001b[1;91mdialog\u001b[0m is less then Terminator and it is not because there is \u001b[1;91mso\u001b[0m much action. The characters \u001b[1;91mjust\u001b[0m don't talk. And, then they \u001b[1;91mdon't even\u001b[0m have \u001b[1;91msomething\u001b[0m corny to \u001b[1;91msay\u001b[0m like 'I'll be back.' If my buddy shot this on the weekend, I'd cheer for him, because it is \u001b[1;91mquite\u001b[0m a feat to figure out the \u001b[1;91mcamera\u001b[0m controls. To pay \u001b[1;91mmoney\u001b[0m to \u001b[1;91mrent\u001b[0m this as a DVD is \u001b[1;91mtotally\u001b[0m \u001b[1;91minappropriate\u001b[0m.<br /><br />The one \u001b[1;91mthing\u001b[0m that is a little funny is the extra with the director telling, how they local police \u001b[1;91mdidn't\u001b[0m realize that they were shooting and treated them like a \u001b[1;91mrandom\u001b[0m guy \u001b[1;91mwalking around\u001b[0m with a gun. If they'd have filmed that, I'd be sure it \u001b[1;91mwould\u001b[0m be more \u001b[1;91mfun\u001b[0m to watch then the movie.\n",
      "\n",
      "Negative Review 1:\n",
      "I \u001b[1;91mhated\u001b[0m \u001b[1;91mthis crap\u001b[0m, every Friday as part of tgif it was on, and consistently \u001b[1;91msucked\u001b[0m big time with \u001b[1;91mstupidity\u001b[0m \u001b[1;91meach\u001b[0m and every week. If you \u001b[1;91mwant\u001b[0m to see \u001b[1;91msomething\u001b[0m funny go watch \\No On Would Tell\\\" Starring Candice Cameron and Fred Savage, it really is \u001b[1;91mhilarious\u001b[0m, \u001b[1;91mshows\u001b[0m exactly \u001b[1;91mwhy\u001b[0m no one ever goes on to a good film career after \u001b[1;91mdoing\u001b[0m a \u001b[1;91mterrible\u001b[0m TV show. This show really \u001b[1;91mmakes\u001b[0m me sick, I hate those kids, and bob saget needs to go jump \u001b[1;91moff\u001b[0m a bridge for ever \u001b[1;91mmaking\u001b[0m this crapfest. I've seen funner stuff everywhere \u001b[1;91melse\u001b[0m but here. I AHet \u001b[1;91mwriting\u001b[0m 10 \u001b[1;91mlines\u001b[0m! Watch 'full house' to see the \u001b[1;91mleast\u001b[0m humanity has to offer in the way of arts and entertainment.\"\n",
      "\n",
      "Negative Review 2:\n",
      "Trite and tiring, the one-liners almost \u001b[1;91mmade\u001b[0m me cry. My \u001b[1;91m4\u001b[0m year old left the room and \u001b[1;91mended up\u001b[0m \u001b[1;91mdoing\u001b[0m a puzzle. I \u001b[1;91mdon't\u001b[0m know what age group this was written for, but the writer himself/herself \u001b[1;91mdidn't\u001b[0m \u001b[1;91meven\u001b[0m \u001b[1;91mwant\u001b[0m credit. As for the song, it's \u001b[1;91mmildly\u001b[0m amusing. At \u001b[1;91mleast\u001b[0m it was a decade ago. There are many Christmas \u001b[1;91mmovies\u001b[0m to watch. Although I've seen \u001b[1;91msome\u001b[0m many more \u001b[1;91mtimes\u001b[0m \u001b[1;91mthan this\u001b[0m, they are \u001b[1;91mstill\u001b[0m \u001b[1;91menjoyable\u001b[0m. Whenever this comes on, I \u001b[1;91mtry\u001b[0m to encourage my child to watch \u001b[1;91msomething\u001b[0m \u001b[1;91melse\u001b[0m. One \u001b[1;91mpositive\u001b[0m note, that allowed a vote of \u001b[1;91m2\u001b[0m \u001b[1;91minstead\u001b[0m of \u001b[1;91m1\u001b[0m, is that it encourages good moral values. That \u001b[1;91mwould\u001b[0m have \u001b[1;91mbeen\u001b[0m encouraging, \u001b[1;91mif\u001b[0m anyone were \u001b[1;91mwatching\u001b[0m.\n",
      "\n",
      "Negative Review 3:\n",
      "I admit to liking a lot of the so-called \\frat-pack\\\" \u001b[1;91mmovies\u001b[0m. No matter \u001b[1;91mhow bad\u001b[0m they are, I can find something to \u001b[1;91mlike\u001b[0m about Ben Stiller or Owen Wilson or Vince Vaughn or Will Ferrell or Jack Black. But \\\"Envy\\\" \u001b[1;91mjust\u001b[0m left me \u001b[1;91mabout as\u001b[0m cold as the white horse that Ben disposed of. This time, it's Ben and Jack Black as a couple of nutty neighbors, \u001b[1;91mone of\u001b[0m whom (Black) discovers a aerosol spray to make animal poop disappear and becomes incredibly wealthy while the other (Stiller) writhes in envy. That's \u001b[1;91msupposedly\u001b[0m the plot, \u001b[1;91mbut then\u001b[0m it veers \u001b[1;91moff\u001b[0m in other directions that \u001b[1;91mdon't\u001b[0m really make much sense.<br /><br />I \u001b[1;91mguess\u001b[0m the 'Vapoorize' thing is \u001b[1;91msort\u001b[0m of amusing \u001b[1;91mat first\u001b[0m. The \u001b[1;91mproblem\u001b[0m is, they \u001b[1;91mtry\u001b[0m to sustain the gag for the whole picture (Black has a license plate that reads 'Caca King') and it gets fairly tiresome. But even Ben and Jack are used poorly; the energy level for \u001b[1;91mboth\u001b[0m of their performances \u001b[1;91mseems\u001b[0m significantly dialed down. The two \u001b[1;91mbest performances\u001b[0m \u001b[1;91mby\u001b[0m far are Rachel Weisz and Chris Walken. Walken's neo-hippie-dippie guy is so offbeat and \u001b[1;91mso well\u001b[0m-modulated a \u001b[1;91mperformance\u001b[0m that it really never suggests \u001b[1;91many\u001b[0m of Walken's other \u001b[1;91mfamiliar\u001b[0m nutcase characters. It's completely \u001b[1;91munique\u001b[0m, \u001b[1;91myet\u001b[0m comes across as unmistakably Walken. And Weisz is about the best actress in the business that nobody knows about. Even with limited screen time, she \u001b[1;91mstill\u001b[0m dominates every scene she's in.<br /><br />The whole crux of the so-called \u001b[1;91mdrama\u001b[0m is that Ben, in a jealous drunken stupor, accidentally shoots Jack's prize white stallion, and then goes to \u001b[1;91mridiculous\u001b[0m lengths to \u001b[1;91mcover\u001b[0m it up, fearing his best friend will find out and \u001b[1;91mcut\u001b[0m him dead. But the plot twist \u001b[1;91misn't\u001b[0m \u001b[1;91mbelievable\u001b[0m \u001b[1;91mbecause\u001b[0m \u001b[1;91mthere's\u001b[0m \u001b[1;91mnothing\u001b[0m about Jack's character to indicate that he \u001b[1;91mwould\u001b[0m \u001b[1;91mdo\u001b[0m such a thing. He \u001b[1;91mplays\u001b[0m such a \u001b[1;91msweet\u001b[0m guy that it renders the whole excruciating horse chase null and void. You discount it completely. It's all filler. And \u001b[1;91mwhat's\u001b[0m the \u001b[1;91mpoint\u001b[0m of the out-of-control merry-go-round, \u001b[1;91mexcept\u001b[0m that Barry Levinson wants us to know that he's seen \\\"Strangers on a Train\\\"? The screenplay is \u001b[1;91mpainfully\u001b[0m bad and the \u001b[1;91macting\u001b[0m of the two leads poorly directed. Someone with Levinson's track record \u001b[1;91mshould\u001b[0m know \u001b[1;91mbetter\u001b[0m. Maybe someone will invent something to make this film disappear. Oh, wait, they already have.\"\n",
      "\n",
      "Negative Review 4:\n",
      "Now we know where they \u001b[1;91mgot\u001b[0m the \u001b[1;91midea\u001b[0m of Snakes on a Plane. To put it bluntly, do not pay to \u001b[1;91msee\u001b[0m this movie. If you \u001b[1;91mreally\u001b[0m \u001b[1;91mwant\u001b[0m \u001b[1;91mto waste\u001b[0m \u001b[1;91m90\u001b[0m \u001b[1;91mminutes\u001b[0m of your \u001b[1;91mlife\u001b[0m, then \u001b[1;91meither\u001b[0m catch it on cable, or \u001b[1;91mget\u001b[0m it as a free pick from NetFlix or Blockbuster. Do not pay to rent this. If you do pay to rent this, then you are one \u001b[1;91mstupid\u001b[0m individual. The \u001b[1;91macting\u001b[0m was awful, the \u001b[1;91mplot\u001b[0m was awful, everything was awful \u001b[1;91mexcept\u001b[0m for the snakes. Whether they were real or CGI generated, they did look pretty good. But that being said, \u001b[1;91mstill\u001b[0m this movie has \u001b[1;91mto be\u001b[0m \u001b[1;91mone of\u001b[0m the \u001b[1;91mworst\u001b[0m \u001b[1;91mmovies\u001b[0m I have \u001b[1;91mever seen\u001b[0m. Even the nude dancing scene was \u001b[1;91mpretty bad\u001b[0m that I \u001b[1;91mactually\u001b[0m fast forwarded through that. Don't \u001b[1;91msat\u001b[0m I did not \u001b[1;91mwarn\u001b[0m you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(negative_reviews)):\n",
    "    print(f'Negative Review {i}:')\n",
    "    negative_review = negative_reviews[i]\n",
    "    highlight_words(r'\\033[1;91m\\1\\033[0m', negative_review, negative_words_in_reviews[i])\n",
    "    # highlight_words(r'\\033[1;91m\\1\\031[0m', negative_review, negative_words_in_reviews[i])\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f4QgIi7xTcL"
   },
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C33WgEFxTcM"
   },
   "source": [
    "The interpretability approach that we took was using bag of words. We made use of the the model and BERT embeddings that we generated for Split 1 in Part 1. Then in Part 2 we used a linear regression model so that we could map the BERT embeddings to fit the same dimensionality as the OpenAI embeddings and the model we training in Part 1. We then chose 5 random positive/negative reviews from split 1, and then highlighted words in each review that showed up in the top 2000 words from the BERT embeddings. In general, the highlighted words seem to reflect positive and negative sentiments. Although there are some words highlighted in the positive reviews that seem more neutral or negative (if, both, man who, etc.) And there are some words highlighted in the negative reviews that also seem more netural or positive (2, paper, to be, etc.) There are limitations with this approach as it disregards the word order and context. Another limitation (discussed on CampusWire) is that due to collinearity among features, some words may have negative coefficients even if their marginal effects are positive. The bag of words approach also can't handle out of vocabulary words that are in the test set but don't appear in the training set."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
